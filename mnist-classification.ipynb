{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ramsesmdlc/mnist-linear-classification?scriptVersionId=140225983\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# __0. Libraries__","metadata":{}},{"cell_type":"code","source":"#Import Libraries \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport matplotlib as mpl # data visualization\nimport matplotlib.pyplot as plt # data visualization (\"pyplot module\", a.k.a. \"plt\")\n\n    #Each \"pyplot\" function makes some change to a figure: e.g., creates a figure, creates a...\n    #...plotting area in a figure, plots some lines in a plotting area, decorates the plot with...\n    #...labels, etc. The various plots we can utilize using Pyplot are Line Plot, Histogram, Scatter,...\n    #...3D Plot, Image, Contour, and Polar.\n\n    #Binary Classifier\nfrom sklearn.linear_model import SGDClassifier # Machine Learning Model (Linear Classification)\nfrom sklearn.model_selection import cross_val_predict #Generating predictions from the Training Set\n\n    #Performance Measures used on Classification Models\nfrom sklearn.metrics import classification_report #Classification Report (Confusion Matrix, Precision score, Recall score, and F1 Score)\nfrom sklearn.metrics import confusion_matrix #Performance Measure (Confusion Matrix)\nfrom sklearn.metrics import ConfusionMatrixDisplay #Performance Measure (Plot Confusion Matrix)\n\nfrom sklearn.metrics import precision_score, recall_score #Performance Measure (Precision and Recall Score)\nfrom sklearn.metrics import f1_score #Performance Measure (F1 Score)\nfrom sklearn.metrics import precision_recall_curve #Performance Measure (Precision Recall Curve)\nfrom sklearn.metrics import roc_curve #Performance Measure (ROC Curve)\nfrom sklearn.metrics import roc_auc_score #Performance Measure (ROC Curve AUC Score)\n\n    #Multiclass Classification\nfrom sklearn.svm import SVC #Support Vector Classification (from Support Vector Machines)\nfrom sklearn.multiclass import OneVsRestClassifier #One-vs-the-rest (OvR) multiclass classifier","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:10.749628Z","iopub.execute_input":"2023-08-16T20:08:10.750056Z","iopub.status.idle":"2023-08-16T20:08:10.759112Z","shell.execute_reply.started":"2023-08-16T20:08:10.750027Z","shell.execute_reply":"2023-08-16T20:08:10.757698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __1. Important__\n\nType of machine learning system to build:\n\n1. Supervised Learning: __Linear Classification__\n2. Batch Learning (also called \"offline learning\")\n3. Model-based learning\n\nGoal:\n1. __It is to take an image of a handwritten single digit, and determine what that digit is.__\n\n2. __Metric__: This competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).","metadata":{}},{"cell_type":"markdown","source":"# __2. Loading the Data - Training Dataset__","metadata":{}},{"cell_type":"code","source":"training_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:10.760828Z","iopub.execute_input":"2023-08-16T20:08:10.761214Z","iopub.status.idle":"2023-08-16T20:08:13.947113Z","shell.execute_reply.started":"2023-08-16T20:08:10.761187Z","shell.execute_reply":"2023-08-16T20:08:13.946267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __2.1. General Information__","metadata":{}},{"cell_type":"markdown","source":"1. The data file contain __gray-scale images__ of hand-drawn digits, from 0 through 9.\n\n2. Each image is __28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.__\n\n> __Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel__, with higher numbers meaning darker. \n\n> This pixel-value is an integer between 0 and 255, inclusive.\n\n> __Each pixel is a feature__.\n\n3. The __training data set, has 785 columns__. The first column, called \"label\", is the digit that was drawn by the user. The __rest of the columns contain the pixel-values of the associated image__.\n\n4. __Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive__. ","metadata":{}},{"cell_type":"markdown","source":"# __3. Exploratory Data Analysis (EDA) - Training Dataset__","metadata":{}},{"cell_type":"code","source":"def EDA(training_data):  \n  # Print the \"shape\" of the dataframe\n  print(\"\\n\" +'\\033[1m','\\033[94m',\"Shape of the dataframe:\",'\\033[0m', training_data.shape, \"\\n\")\n    \n  # Print the \"keys\" of the dataframe\n  print(\"\\n\" +'\\033[1m','\\033[94m',\"Keys of the dataframe:\",'\\033[0m', \"\\n\")\n  print(training_data.keys())  \n    \n  # Print the \"head\" of the dataframe\n  print(\"\\n\" +'\\033[1m','\\033[94m',\"Head of the dataframe:\",'\\033[0m', \"\\n\")\n  print(training_data.head())\n  \n  # Print the \"general information\" of the dataframe\n  print(\"\\n\", '\\033[1m','\\033[94m',\"Information of the dataframe:\",'\\033[0m', \"\\n\")\n  training_data.info()\n \n  # Print the \"number and percentage\" of missing values per column\" of the dataframe\n  print(\"\\n\" +'\\033[1m','\\033[94m', \"Number and percentage of missing values per column of the dataframe:\", '\\033[0m' + \"\\n\")\n  missing = training_data.isnull().sum()\n  percent = missing / training_data.shape[0] * 100\n  print(pd.concat([missing, percent], axis=1, keys=[\"Missing\", \"Percent\"]))\n    \n# Call the function\nEDA(training_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:13.949076Z","iopub.execute_input":"2023-08-16T20:08:13.949673Z","iopub.status.idle":"2023-08-16T20:08:14.032335Z","shell.execute_reply.started":"2023-08-16T20:08:13.949643Z","shell.execute_reply":"2023-08-16T20:08:14.031201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __3.1. Adjusting the Training Dataset__\n\n1. If we want to see the data (as an image), we should consider what is described in the section \"2.1. General Information\" (i.e. __the pixel's values of the image of every number is entirely displayed in one single row composed by 784 values); therefore, it is necessary rescaling the data (i.e. rescale each row of the dataframe from a shape of {1 x 784} pixels to {28 x 28} pixels.__\n\n> For example: To locate this pixel (\"pixel31\") on the image of a specific number, \"pixel31\" indicates the pixel is in the fourth column from the left, and the second row from the top.\n\n2. Before applying the \"reshape\" function, we need to transform our dataset from \"Pandas dataframe\" to a \"Numpy Array\", because the \"Numpy Array\" has the function \"reshape\" available.","metadata":{}},{"cell_type":"code","source":"print(\"\\n\", '\\033[1m','\\033[94m',\"Information (Shape and Type) of the Dataframe (Y and X variables separatelly):\",'\\033[0m', \"\\n\")\n#Separating the Y and X values (i.e. dependent and independent variables)\nY_training, X_training = training_data[\"label\"],training_data.iloc[:, 1:785] \n\n#Printing the \"shape\" of the dependent and independent variables (separately)\nprint(Y_training.shape)\nprint(X_training.shape)\n\n#Printing the \"type\" of the dependent and independent variables (separately)\nprint(type(Y_training))\nprint(type(X_training))\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Information (Shape) of the Numpy Array (Y and X variables separatelly):\",'\\033[0m', \"\\n\")\n#Transforming the \"independent variables\" (from Pandas Dataframe to Numpy Array)\narray_X = X_training.to_numpy()\nprint(array_X)\n\n#Transforming the \"dependent variables\" (from Pandas Dataframe to Numpy Array)\narray_Y = Y_training.to_numpy()\nprint(array_Y)\n\n#Printing the \"shape\" of the independent variables (as a Numpy Array)\nprint(array_X.shape) \n\n#Printing the \"shape\" of the independent variables (as a Numpy Array)\nprint(array_Y.shape) ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:14.033559Z","iopub.execute_input":"2023-08-16T20:08:14.033984Z","iopub.status.idle":"2023-08-16T20:08:14.044154Z","shell.execute_reply.started":"2023-08-16T20:08:14.033944Z","shell.execute_reply":"2023-08-16T20:08:14.043103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing the image of a number (just to check!)\n\n#Digit of row N°5, counting from 0,1,2,3,4 (without rescaling)\n    #If we see the section \"3. Exploratory Data Analysis (EDA) - Training Dataset\", the output should\n    #...be the number zero (0)\ndigitx1 = array_X[4]\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Target Value of the Image:\",'\\033[0m', \"\\n\")\nprint(Y_training[4])\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Value of every pixel of the image (28 x 28 pixels):\",'\\033[0m', \"\\n\")\nprint(digitx1)\n\n#Digit of row N°5 (with rescaling)\ndigitx1_image = digitx1.reshape(28, 28)\n\n#The matplotlibe function \"imshow\":\n    #Display data as an image.\n    #The input may either be actual RGB(A) data, or 2D scalar data, which will be rendered as a...\n    #...pseudocolor image. For displaying a grayscale image set up the colormapping using the...\n    #...parameters cmap='gray', vmin=0, vmax=255.\n    #The number of pixels used to render an image is set by the Axes size and the dpi of the figure.\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Image (28 x 28 pixels):\",'\\033[0m', \"\\n\")\nplt.imshow(digitx1_image,cmap=\"binary\")\nplt.axis(\"off\")\nplt.show ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:14.047054Z","iopub.execute_input":"2023-08-16T20:08:14.04791Z","iopub.status.idle":"2023-08-16T20:08:14.158368Z","shell.execute_reply.started":"2023-08-16T20:08:14.047871Z","shell.execute_reply":"2023-08-16T20:08:14.157029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing the image of a number (just to check!)\n\n#Digit of row N°3, counting from 0,1,2 (without rescaling)\n    #If we see the section \"3. Exploratory Data Analysis (EDA) - Training Dataset\", the output should\n    #...be the number one (1)\ndigitx2 = array_X[2]\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Target Value of the Image:\",'\\033[0m', \"\\n\")\nprint(Y_training[2])\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Value of every pixel of the image (28 x 28 pixels):\",'\\033[0m', \"\\n\")\nprint(digitx2)\n\n#Digit of row N°3 (with rescaling)\ndigitx2_image = digitx2.reshape(28, 28)\n\n#The matplotlibe function \"imshow\":\n    #Display data as an image.\n    #The input may either be actual RGB(A) data, or 2D scalar data, which will be rendered as a...\n    #...pseudocolor image. For displaying a grayscale image set up the colormapping using the...\n    #...parameters cmap='gray', vmin=0, vmax=255.\n    #The number of pixels used to render an image is set by the Axes size and the dpi of the figure.\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Image (28 x 28 pixels):\",'\\033[0m', \"\\n\")\nplt.imshow(digitx2_image,cmap=\"binary\")\nplt.axis(\"off\")\nplt.show ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:14.159729Z","iopub.execute_input":"2023-08-16T20:08:14.160345Z","iopub.status.idle":"2023-08-16T20:08:14.308485Z","shell.execute_reply.started":"2023-08-16T20:08:14.160314Z","shell.execute_reply":"2023-08-16T20:08:14.30701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __4. Training Model (Binary Linear Classifier) - Training Dataset__\n\n__Key ideas (Part 1):__\n\n1. __Binary Classifier__: distinguish between two classes. Some models are:\n\n> Logistic Regression\n\n> Support Vector Machines\n\n> SGD Classifier (i.e., __it is a \"Linear Classifier\"__ + __application of the optimization technique called \"Stochastic Gradient Descent\").__\n\n2. __Multiclass Classifier__: distinguish between two or more classes. Some models are:\n\n> SGD Classifier (i.e., __it is a \"Linear Classifier\"__ + __application of the optimization technique called \"Stochastic Gradient Descent\").__\n\n> Random Forests Classifiers\n\n> Naive Bayes Classifiers","metadata":{}},{"cell_type":"markdown","source":"__Key ideas (Part 2):__\n\n***Linear Classifier:*** \n\n1. It is a type of machine learning __algorithm__ that can __classify data__ into different categories __based on a \"linear function of the features\"__.\n\n> Extremely Important: __\"linear function of the features, not the weights\".__\n\n> __Linear function of the features__: it means that the __output__ of the function is the __weighted sum of the input features__ plus a bias term. It does __not involve any nonlinear transformation of the features__ (cubic, quadratic, exponential,...). __It depends of the input data__. It is useful to __model relationships between the input and outputs__.\n\n> __Linear function of the weights__: it means that the __output__ of the function is a __linear combination of the weights__. It does __not involve any nonlinear transformation of the weights. It depends of the model's parameter__. It is useful to __regularize or constrain the model complexity (using Norm L1, Norm L2, etc.)__\n\n2. It can __learn a set of \"weights\" and \"a threshold\"__ that can separate the instances into different classes.\n\n> __Threshold__: it is a value that __determines which class a data point belongs to based on its distance from the decision boundary__. It can be adjusted to optimize the performance of the classifier. \n\n3. It __can be represented by a line, a plane, or a hyperplane__ in the feature space, depending on the number of features. \n\n> __The \"threshold\" is not__ a __\"line, a plane, or a hyperplane\"__\n\n> __The \"decision boundary\" is__ a __\"line, a plane, or a hyperplane\"__ that divides the input space into regions corresponding to different classes.\n\n> The __decision boundary of a linear classifier__ is the __set of points that are equally distant from two or more classes.__\n\n> __Hyperplane__: it is a generalization of a line, plane, etc. It is a __subspace of one dimension less than the ambient space__.\n \n4. It can be trained using different methods: __Linear Discriminant Analysis (LDA)__, Naive Bayes, or __Stochastic Gradient Descent (SGD)__\n\n5. __Because its linearity in the features,__ the advatanges and disavantages of a \"Linear Classifier\" are the followings:\n\n> Advantages: fast, simple, and scalable to high-dimensional data.\n\n> Disadvantages: they may not be able to capture complex nonlinear patterns in the data, and they may be sensitive to outliers and noise.","metadata":{}},{"cell_type":"markdown","source":"__Key ideas (Part 3):__\n\n***Stochastic Gradient Descent:*** \n\n1. (__SGD__) Stochastic gradient descent and (__LDA__) linear discriminant analysis __are two different concepts__.\n\n2. LDA: __it is a classification method__ that can be used to separate data into different classes based on a linear combination of features that best separates the classes by maximizing the ratio of between-class variance to within-class variance. \n\n3. SGD: __it is an optimization method__ that can be used to train various models, such as linear regression, logistic regression, support vector machines or neural networks.\n\n> It does __not correspond__ to a specific family of machine learning models.\n\n> It is an __iterative method that updates the parameters of a model__ by using the data to estimate the gradient of the cost function. \n\n> It can introduce noise and instability in the optimization process, and __may require careful tuning of the learning rate and other hyperparameters__, and it is __sensitive to feature scaling.__\n\n4. We can __use SGD with__ two different types of machine learning models such as __\"classifiers\"__ and __\"regressors\"__.\n\n> \"Linear classifier\" and a \"Regressor\" solve different kinds of problems.  \n\n5. The main differences between a \"Linear classifier\" and a \"Regressor\" are the __type of output__ they produce, the __way they measure the error or loss of their predictions.\n\n> The __\"Linear classifier\" model__ can __predict a discrete class label__ for a given input (for instance, output class labels such as positive or not negative, sushi or pizza, etc). It __uses metrics such as accuracy, precision, recall, or F1-score__ to evaluate its performance in the classification tasks.\n\n> The __\"Regressor Model\"__ can __predict a continuous value__ for a given input, such as pressure and weight, etc. It outputs a numerical value, which can be more or less close to the true value, and uses __metrics such as mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE)__ to evaluate its performance. A regressor uses a function (such as a line, a curve, or a surface) to fit the data and estimate the output value for any given input.","metadata":{}},{"cell_type":"code","source":"#Considering a \"Binary Linear Classifier\" approach, we are going to build a model to detect the number nine (9).\n\n#Model to use: SGDClassifier. It is named in our code as \"sgd_bin_clf\"\n    #SGDClassifier (\"Linear Classifier\" + optimization technique called \"Stochastic Gradient Descent\")\n\n#Defining the two classes (because we are talking about a \"Binary Classifier\"):\n    #Class N°1: all numbers that are nine (9) in the dataset of images.\n    #Class N°2: all numbers that are not nine (9) in the dataset of images.\n\n#Defining Class N°1: \n    #It works like a boolean (True or False), without deleting any row. Jut putting \"True\" the rows...\n    #...with the \"target value nine (9)\".\ny_train_9 = (Y_training == 9) \n\n#SGDClassifier \n    #This code implements a stochastic gradient descent learning routine which supports different...\n    #...loss functions and penalties for classification.\n    \n    #It is important to shuffle the training data before fitting the model.\n    \n    #random_state\n        #It is an argument from the function \"SGDClassifier\"\n        #It is used for \"shuffling the data\", when \"shuffle\" is set to \"True\".\n        #It is useful for reproducibility of outputs across multiple function calls.\n        #If you dont want to change the global \"seed\" value and only want to set the \"state\" for one task,..\n        #...random_state is used. \n    #random.seed()    \n        #It is a function, not an argument.\n        #It is little bit differente from the argument \"random_state\"\n        #The effect of setting the \"seed\" is global as it will end up effecting all functions.\n\nsgd_bin_clf = SGDClassifier(random_state = 42)\n    #Both sets \"X_training\" and \"y_train_9\" have the same number of rows (i.e., 42000)\nsgd_bin_clf.fit(X_training, y_train_9)\n\n#The output of our model (based on the training set) apparently are good, considering that we should...\n#...not detect the image of number nine (9) and the first three outputs fo the array show...\n#...\"False, False, False\" because those number's images belong to the numbers \"1,0,1\" (accordingly...\n#...to section \"3. Exploratory Data Analysis (EDA) - Training Dataset\")\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Output of the Training Phase:\",'\\033[0m', \"\\n\")\nsgd_bin_clf.predict(X_training)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:14.310915Z","iopub.execute_input":"2023-08-16T20:08:14.311849Z","iopub.status.idle":"2023-08-16T20:08:27.443894Z","shell.execute_reply.started":"2023-08-16T20:08:14.311796Z","shell.execute_reply":"2023-08-16T20:08:27.442497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __5. Performance Measures - Training Model (Binary Classifier) - Training Dataset__","metadata":{}},{"cell_type":"markdown","source":"## __5.1. Confusion Matrix__\n\n__Key ideas:__\n\n1. The basic idea is to __count the number of time an item (instance) of a specific class (for example: Class W) is classified as another class (for example: Class H)__. In other words, this method allows us to extract information about the model's performance and __determine if the model is \"confused\" in discriminating between classes.__\n\n2. This method can be __used for \"Binary Classification\" and \"Multiclass Classification\".__\n\n3. Each __row__ of the \"Confusion Matrix\" __represents \"True Class = Ground Truth\".__\n\n4. Each __column__ of the \"Confusion Matrix\" __represents \"Predicted Class\".__\n\n5. In a ideal world, we want a Confusion Matrix that shows maximum metrics of \"True Positive\" and \"True Negative\" and minimum metrics of \"False Positive\" and \"False Negative\".","metadata":{}},{"cell_type":"code","source":"#Step 1 (Cross-Validation)\n    #It generate \"cross-validated estimates\" (also called \"pseudo-predictions\") for each input data point...\n    #...In other words, this function returns for \"each element in the input\", the \"prediction\" that...\n    #...was obtained for that \"element when it was in the test set\". during the \"Cross-Validation process\".\n        #In this step we are going to generate \"predictions\" using the training data.        \n        #Then those \"predictions\" will be compared to the known target values (because all data values..\n        #...used in this Step 1 belong to the \"training set\").\n    \n    #cv:\n        #The data is splitted according to the \"cv\" parameter. \n        #It is the \"cross-validation generator\" (default=None).\n        #Determines the \"cross-validation\" splitting strategy. \n        #Possible inputs for \"cv\" are:\n            #None (to use the default 5-fold cross validation).\n            #int (to specify the number of folds in a \"\"(Stratified)KFold\".\n        #If the \"estimator\" (also called model, which in our case is called \"sgd_bin_clf\") is a...\n        #...\"classifier\" and the \"dependent variable\" is either \"binary or multiclass\", the...\n        #...\"StratifiedKFold\" is used. \n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Predictions from CV Process:\",'\\033[0m', \"\\n\")\ny_train_pred = cross_val_predict(sgd_bin_clf, X_training, y_train_9, cv=5)\nprint(y_train_pred)\n    \n#Step 2 (Confusion Matrix)\n    #We are going to generate the \"Confusion Matrix\" using the \"training data\" and the \"predictions\"...\n    #...(from the previous step; i.e., from the Step 1)\n    \n    #Confusion Matrix\n        #Compute \"confusion matrix\" to evaluate the \"accuracy\" of a classification.\n        #By definition a confusion matrix \"C\" is such that \"Cij\" is equal to the number of \"observations...\n        #...known\" to be in group \"i\" (rows) and \"predicted\" to be in group \"j\" (columns)\n        #In binary classification, the count is:\n            #true negatives = C00 (left upper quadrant)\n            #false negatives = C10 (left lower quadrant)\n            #true positives = C11 (right lower quadrant)\n            #false positives = C01 (right upper quadrant)\n\n#The output of the confusion matrix will shows us that there are:\n    # 36771 images classified correctly as non-9s\n    # 1366  images classified wrongly as non-9s\n    # 1041  images classified wrongly as 9s\n    # 2822  images classified correclty as 9s\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Confusion Matrix:\",'\\033[0m', \"\\n\")\ncm = confusion_matrix(y_train_9, y_train_pred)\nprint(cm)\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Plot of the Confusion Matrix:\",'\\033[0m', \"\\n\")\ndisp_confusion_matrix = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=sgd_bin_clf.classes_)\ndisp_confusion_matrix.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:08:27.44621Z","iopub.execute_input":"2023-08-16T20:08:27.447115Z","iopub.status.idle":"2023-08-16T20:09:30.498998Z","shell.execute_reply.started":"2023-08-16T20:08:27.447062Z","shell.execute_reply":"2023-08-16T20:09:30.498159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.2. Precision and Recall__\n\n__Key ideas:__\n\n1. __These metrics are considered more precise than the information from \"Confusion Matrix\"__ described on section \"5.1. Confusion Matrix\"; therefore, we are going to use them. By definition a confusion matrix \"C\" is such that \"Cij\" is equal to the number of \"observations known\" to be in group \"i\" (rows) and \"predicted\" to be in group \"j\" (columns):\n \n > true negatives = C00 (left upper quadrant)\n \n > false negatives = C10 (left lower quadrant)\n \n > true positives = C11 (right lower quadrant)\n \n > false positives = C01 (right upper quadrant)\n\n2. __Precision__: this metric allow us to know how reliable is a model when it tries to classify \"true positive\" values. In other words, with this metric we are going to answer the following question: __What portion of all values classified as \"positive\" (i.e., \"false positive\" + \"true positive\") are really \"true positive\" values?__\n\n> We should try to maximize this metric.\n\n> This metric is __based on \"detected values\" (\"True Positive\" and \"False Positive\")__. In other words, this metric takes into account a group of values (correctly and wrongly) classified as positive. \n\n> __Precision Equation = TP / (TP+FP)__; where \"TP: True Positive\" and \"FP: False Positive\".\n\n3. __Recall__: this metric allow us to know how many \"positive values\" were detected among \"all possible positive values\" (i.e., \"detected positive\" values and \"non-detected postive\" values).\n\n> It is __also called \"Sensitivity\" or \"True Positive Rate\" (TPR).__\n\n> \"False positive\" values, which are considered \"negative samples\" wrongly classified, are neglected by this metric. \n\n> This metric is __based on \"detected values\" and \"non-detected postive\" values.__\n\n> __Recall Equation = TP / (TP+FN)__; where \"TP: True Positive\" and \"FN: False Negative\".\n\n4. Relationship between Precision and Recall metrics\n\n|Precision Score|Recall Score|Comment\n|--|--|--|\n|High|Low|The model correctly classify the \"true positive\" values, but due to the low value of the \"Recall\" metric (i.e., few points of data \"detected as positive\" values), we are computing the \"Precision\" metric with just a few points of data|\n|Low|High|The model correctly detects the \"positive values\", but we need to be careful because the model can also detect too many \"False Positive\" values. Consequently, this impacts the \"Precision\" metric.|","metadata":{}},{"cell_type":"code","source":"#Computation Precision and Recall\n\n#Precision\n    #The model will show (output) a Precision metric of 73.05% (i.e., the model will be correct...\n    #...about a class assignment {in this case the number nine (9)} 73.05% of the time.)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Precision Score:\",'\\033[0m', \"\\n\")\nprint(precision_score(y_train_9, y_train_pred))\n\n#Recall\n    #The model will show (output) a Recall metric of 67.38% (i.e., the model will detect...\n    #...67.38% of \"positive values\" {in this case the number nine (9)} among \"all possible positive...\n    #...values\" (i.e., \"detected positive\" values and \"non-detected postive\" values). \nprint(\"\\n\", '\\033[1m','\\033[94m',\"Recall Score:\",'\\033[0m', \"\\n\")\nprint(recall_score(y_train_9, y_train_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:09:30.500312Z","iopub.execute_input":"2023-08-16T20:09:30.501066Z","iopub.status.idle":"2023-08-16T20:09:30.539934Z","shell.execute_reply.started":"2023-08-16T20:09:30.501036Z","shell.execute_reply":"2023-08-16T20:09:30.538761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.3. F1 Score__\n\n__Key ideas:__\n\n1. This metric is very __useful when comparing models.__\n\n2. It works like a __mean of the \"Precision\" and \"Recall\" metrics.__\n\n3. It __ranges from 0 (low values of both \"Precision\" and \"Recall\" metric.) to +1 (high values of both \"Precision\" and \"Recall\" metric.)__ \n\n4. It __favours models with similar \"Precision\" and \"Recall\" scores__. It is important to take into account the following:\n\n> In __some contexts it is better to have higher \"Recall\" scores__. For instance: in self-driving, detecting pedestrians with the cameras and sensors.\n\n> In __some contexts it is better to have higher \"Precision\" scores__. For instance: detecting cancer or HIV on patients.\n\n5. __F1 Equation = (2 x Precision x Recall)/(Precision + Recall)__","metadata":{}},{"cell_type":"code","source":"#F1\n    #The model will show (output) a F1 metric of 0.70.\nprint(\"\\n\", '\\033[1m','\\033[94m',\"F1 Score:\",'\\033[0m', \"\\n\")\nprint(f1_score(y_train_9, y_train_pred))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:09:30.541665Z","iopub.execute_input":"2023-08-16T20:09:30.542136Z","iopub.status.idle":"2023-08-16T20:09:30.564702Z","shell.execute_reply.started":"2023-08-16T20:09:30.542097Z","shell.execute_reply":"2023-08-16T20:09:30.563634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.Extra. Classification Report (Confusion Matrix, Precision score, Recall score, and F1 score)__\n\n***Extremely Important: with this \"Classification Report\" code we can save a lot of time. To be specific, we could avoid doing the sections***:\n\n> 5.1. Confusion Matrix\n\n> 5.2. Precision and Recall\n\n> 5.3. F1 Score\n\n1. __It will show us a summary of the most used performance metrics in classification__ algorithms. We will see:\n\n> Confusion Matrix, Precision score, Recall score, F1 score\n\n2. The __\"Confusion Matrix values\"__ are the same that we have seen in the section \"5.1. Confusion Matrix\" for \"9s class\", to be specific the values of: \n\n> Support \"non-9s\" class: (37812 = 36771 + 1041)\n\n> Support \"9s\" class: (4188 = 1366 + 2822)\n\n3. The output of the confusion matrix will shows us that there are:\n\n|Values|Classification|\n|--|--|\n|36771 images classified correctly as non-9s| true negatives|\n|1366  images classified wrongly as non-9s| false positives|\n|1041  images classified wrongly as 9s| false negatives|\n|2822  images classified correclty as 9s| true positives|\n\n3. The __\"Precision score\"__ is the same that we have seen in the section \"5.2. Precision and Recall\" for \"9s class\", to be specific the value of 0.73\n\n4. The __\"Recall score\"__ is the same that we have seen in the section \"5.2. Precision and Recall\" for \"9s class\", to be specific the value of 0.67\n\n5. The __\"F1 score\"__ is the same that we have seen in the section \"5.3. F1 Score\" for \"9s class\", to be specific the value of 0.70","metadata":{}},{"cell_type":"code","source":"#Classification Report\n    #\"Labels\" of the \"Classification Report\"\nclass_labels = [\"non-9s class\",\"9s class\"]\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Classification report:\",'\\033[0m', \"\\n\")\ncr = classification_report(y_train_9, y_train_pred,target_names=class_labels)\nprint(cr)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:09:30.568969Z","iopub.execute_input":"2023-08-16T20:09:30.569321Z","iopub.status.idle":"2023-08-16T20:09:30.639126Z","shell.execute_reply.started":"2023-08-16T20:09:30.569291Z","shell.execute_reply":"2023-08-16T20:09:30.637953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.4. Precision and Recall Tradeoff (Plot of \"Precision and Recall vs Decision Threshold\")__\n\n__Key ideas:__\n\n1. This detailed analysis is useful if one considers deeply the relevance of the __\"Precision and Recall Tradeoff\" versus the \"Decision Threshold\"__. In some ways this analysis is based on the information described in section \"5.3. F1 Score\".\n\n2. \"Decision Threshold\": it is a value that __determines which class a data point belongs to based on its distance from the decision boundary__; therefore, a change in the \"Decision Threshold\" also means a change in the composition of the quadrants that belong to the \"Confusion Matrix\", and a change in the \"Precision\", \"Recall\" and \"F1\" scores. The following table describes in a better way the __behavior of the scores (\"Precision and Recall\") when the \"Decision Threshold\" is modified__:\n\n|Decision Threshold Value|Precision Score|Recall Score\n|--|--|--|\n|Increase|Increase (not always)|Decrease|\n|Decrease|Decrease|Increase|\n\n3. The __decision for a specific value for the \"Decision Threshold\" will depend on the context of our project__ (as was described in \"5.3. F1 Score\".)\n\n4. __Important__: This graph will help if we are interested in __finding the \"threshold value\" from a specific \"Precision Value\" or \"Recall Value\".__","metadata":{}},{"cell_type":"code","source":"#Finding the \"scores\" of our training dataset.\n\n    #By default, the model \"SGDClassifier\" use a \"Decision Threshold = 0\".\n    \n    #Using the argument (method= \"decision_function\") of the function \"cross_val_predict()\" we are going...\n    #...determine or predict the \"score\" (also called \"confidence scores\") of each instance of our...\n    #...training set. \n        #These \"scores\" are going to be taken into account to make changes in the value...\n        #...of the \"Decision Threshold\".\n        #The score for a sample is proportional to the \"signed distance\" of that sample to the...\n        #hyperplane (i.e. it is just a \"distance\")\n            #The \"signed distance\" to the hyperplane (computed as the dot product between the...\n            #...coefficients and the input sample, plus the intercept).\n    \n    #This method basically returns a Numpy array of scores, in which each score lies...\n    #...to the right or left side of the \"Hyperplane\".\n\n#The output show us positive and negative scores (confidence scores):\n    #Positive scores: it means that the evaluated instance belongs to a the number nine (9) class.\n    #Negative scores: it means that the evaluated instance belongs to a the non-number nine (9) class.\ny_train_scores = cross_val_predict(sgd_bin_clf, X_training, y_train_9, cv=5, method=\"decision_function\")\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Scores of the Training Set:\",'\\033[0m', \"\\n\")\nprint(y_train_scores)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the array of Scores of the Training Set:\",'\\033[0m', \"\\n\")\nprint(y_train_scores.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:09:30.640655Z","iopub.execute_input":"2023-08-16T20:09:30.640978Z","iopub.status.idle":"2023-08-16T20:10:33.253951Z","shell.execute_reply.started":"2023-08-16T20:09:30.640951Z","shell.execute_reply":"2023-08-16T20:10:33.252197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the \"Recall and Precision\" metrics of the training set (i.e. compute precision-recall pairs for...\n#...different thresholds.)\n\n    #Extremely Important: This implementation is restricted to the \"binary classification\".\n    \n    #It is important to remember that in this step is computed the \"Precision and Recall\" based...\n    #...on the \"True binary labels\" (i.e. the set \"y_train_9\") and \"scores of the training set\" (found...\n    #...in the previous step).\n        \n    #In some way, this process is similar to section \"5.2. Precision and Recall\", in which we compute the...\n    #...\"Precision and Recall\" metrics based on the \"training data\" and our \"selected class\" (\"y_train_9\")..\n    #...But in this case we compute the \"Precision and Recall\" metrics based on the scores of ...\n    #...\"training data\" and our \"selected class\" (\"y_train_9\").\n    \n    #Extremely Important: the \"last precision and recall\" values\" are \"1. and 0.\" respectively and do...\n    #... not have a corresponding #threshold\". This ensures that the graph starts on the y axis...\n    #...Therefore, we are going to see the in the output that:\n        #Shape of the array \"precisions\" = 42001 instea of 42000\n        #Shape of the array \"recalls\" = 42001 instea of 42000\n\n    #“thresholds”:\n        #It is a variable that stores an array of values that are used to compute the...\n        #...\"Recall and Precision\".\n        \n        #It contains the values of the \"probability estimates\" for the positive \"class nine (9)\"...\n        #...that correspond to each point on the \"Precision and Recall Tradeoff\" analysis. For example:\n            #The first element of the array \"thresholds\" is 1.8, which means that if the...\n            #...classifier assigns a score of 1.8 or higher to an instance, it will predict that it...\n            #...belongs to \"class nine (9)\". \n            \nprecisions, recalls, thresholds = precision_recall_curve(y_train_9, y_train_scores)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Precision, Recall and Threshold\",'\\033[0m', \"\\n\")\nprint(precisions, recalls, thresholds)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the array precisions:\",'\\033[0m', \"\\n\")\nprint(precisions.shape)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the array recalls:\",'\\033[0m', \"\\n\")\nprint(recalls.shape)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the thresholds:\",'\\033[0m', \"\\n\")\nprint(thresholds.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:33.260681Z","iopub.execute_input":"2023-08-16T20:10:33.261531Z","iopub.status.idle":"2023-08-16T20:10:33.301123Z","shell.execute_reply.started":"2023-08-16T20:10:33.261474Z","shell.execute_reply":"2023-08-16T20:10:33.299492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the \"Recall and Precision versus Decision Threshold\" metrics of the training set...\n#...(i.e. compute precision-recall pairs for different thresholds.)\n\n    #Some portions of the graph will show (output) the \"Precision Line\" going down when the \"threshold\"...\n    #...is raised. It could be strange, but sometimes happen.\n    \n    #Extremely Important: the \"last precision and recall\" values\" are \"1. and 0.\"\" respectively and do...\n    #... not have a corresponding #threshold\". This ensures that the graph starts on the y axis...\n    #...Therefore, we are going to see the in the output that:\n        #Shape of the array \"precisions\" = 42001 instea of 42000\n        #Shape of the array \"recalls\" = 42001 instea of 42000\n        #precisions [:-1]\n        #recalls [:-1]\n    \n    #We are using scores to plot this graph!\n    \ndef plot_precision_recall_threshold (precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1],\"b--\",label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1],\"g-\",label=\"Recall\")\n    plt.title(\"Recall and Precision versus Decision Threshold\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"center left\")\n    \nplot_precision_recall_threshold (precisions, recalls, thresholds)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:33.308033Z","iopub.execute_input":"2023-08-16T20:10:33.30946Z","iopub.status.idle":"2023-08-16T20:10:33.585925Z","shell.execute_reply.started":"2023-08-16T20:10:33.309416Z","shell.execute_reply":"2023-08-16T20:10:33.584813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.5. Precision and Recall Tradeoff (Plot of \"Precision vs Recall\")__\n\n__Key ideas:__\n\n1. The __decision for a specific value for the \"Decision Threshold\" will depend on the context of our project__ (as was described in \"5.3. F1 Score\".)","metadata":{}},{"cell_type":"code","source":"#Plotting the \"Recall vs Precision\" metrics of the training set\n\n    #We are going to take the values generated in section \"5.4. Precision and Recall Tradeoff...\n    #...(Plot of \"Precision and Recall vs Decision Threshold\")\"\n    \n    #We are using scores to plot this graph!\nplt.plot(recalls,precisions)\nplt.title(\"Precision versus Recall\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:33.587744Z","iopub.execute_input":"2023-08-16T20:10:33.588424Z","iopub.status.idle":"2023-08-16T20:10:33.824225Z","shell.execute_reply.started":"2023-08-16T20:10:33.588363Z","shell.execute_reply":"2023-08-16T20:10:33.82315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.6. ROC Curve__\n\n__Key ideas:__\n\n1. The ROC curve __shows how well or the quality of a binary classifier or multiclass classifiers can distinguish between two classes.__ It plots TPR vs FPR at different classification thresholds. \n\n> __Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives__. \n\n2. ___Extremely Important___: Despite the fact that the __\"ROC Curve\" Analysis is similar to the \"Precision and Recall Tradeoff\" analysis, they are applied for different purposes__. The application is described in the following lines:\n\n> Use\"Precision and Recall Tradeoff\" analysis: __if you care about \"False Positives\" or the \"Positive Class\" is rare.__\n\n3. In the case of multiclass classification, TPR or FPR is obtained in the following ways:\n\n> One-vs-Rest (OvR) scheme, that compares each class against all the others (assumed as one).\n\n> One-vs-One (OvO) scheme, that compares every unique pairwise combination of classes.\n\n4. ROC Curve (also called \"Receiver Operating Characteristic\" Curve) plots the \"Recall\", also called \"Sensitivity\" or \"True Positive Rate\" (TPR) against the \"False Positive Rate\" (FPR). \n\n> FPR: ratio of \"negative values\" wrongly classified as \"positive values\".\n\n> TNR: ratio of \"negative values\" correctly classified as \"negatve values\". It is also called \"Specificity\".\n\n> TNR = 1 - FPR","metadata":{}},{"cell_type":"code","source":"#Finding the \"False Positive Rate (FPR) and True Positive Rate (TPR)\" metrics of the training set.\n\n    #Extremely Important: This implementation is restricted to the \"binary classification\".\n    \n    #It is important to remember that in this step is computed the \"False Positive Rate (FPR) and...\n    #...True Positive Rate (TPR)\" based on the \"True binary labels\" (i.e. the set \"y_train_9\")...\n    #...and \"scores of the training set\" (found in the section \"5.4. Precision and Recall...\n    #...Tradeoff (Plot of \"Precision and Recall vs Decision Threshold\")\".\n    \n    #The steps to find \"False Positive Rate (FPR) and True Positive Rate (TPR)\" metrics of the...\n    #...training set are similar to those described on section \"5.4. Precision and Recall...\n    #...Tradeoff (Plot of \"Precision and Recall vs Decision Threshold\")\"\n    \n    #“thresholds2”:\n        #It is a variable that stores an array of values that are used to compute the...\n        #...false positive rate (FPR) and the true positive rate (TPR) for the ROC curve.\n        \n        #It contains the values of the \"probability estimates\" for the positive \"class nine (9)\"...\n        #...that correspond to each point on the ROC curve. For example:\n            #The first element of the array \"thresholds2\" is 1.8, which means that if the...\n            #...classifier assigns a score of 1.8 or higher to an instance, it will predict that it...\n            #...belongs to \"class nine (9)\". \n        \nfp_rate, tp_rate, thresholds2 = roc_curve(y_train_9, y_train_scores)\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"False Positive Rate (FPR), True Positive Rate (TPR), Threshold\",'\\033[0m', \"\\n\")\nprint(fp_rate, tp_rate, thresholds2)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the array False Positive Rate (FPR):\",'\\033[0m', \"\\n\")\nprint(fp_rate.shape)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the array True Positive Rate (TPR):\",'\\033[0m', \"\\n\")\nprint(tp_rate.shape)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Shape of the thresholds:\",'\\033[0m', \"\\n\")\nprint(thresholds2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:33.826275Z","iopub.execute_input":"2023-08-16T20:10:33.827063Z","iopub.status.idle":"2023-08-16T20:10:33.850174Z","shell.execute_reply.started":"2023-08-16T20:10:33.827015Z","shell.execute_reply":"2023-08-16T20:10:33.849021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the \"ROC Curve\"    \n    #ROC curves typically feature \"true positive rate\" (TPR) on the Y axis, and...\n    #...\"false positive rate\" (FPR) on the X axis.\n    #The top left corner of the plot is the “ideal point\" where FPR = 0, and TPR = 1. It means\n        #In essence, a larger \"area under the curve\" (AUC) is usually a good sign that the model...\n        #...has a good performance in the classifying task.\n        #In essence, the \"ROC Curve\" away from the \"Random Classfier\" is usually a good sign that...\n        #...the model has a good performance in the classifying task.\n    #The “steepness” of ROC curves is also important, since it is ideal to maximize the TPR...\n    #...while minimizing the FPR.   \n    \ndef plot_fpr_tpr_threshold (fp_rate, tp_rate, label=None):\n    plt.plot(fp_rate, tp_rate,\"b-\",linewidth = 1, label=\"ROC Curve\")\n    plt.plot([0,1], [0,1],\"g--\",linewidth = 3, label=\"Random Classifier\")\n    plt.title(\"ROC Curve (TPR vs FPR for all possible thresholds)\")\n    plt.xlabel(\"False Positive Rate (FPR)\")\n    plt.ylabel(\"True Positive Rate (TPR)\")\n    plt.legend(loc=\"lower right\")\n    \nplot_fpr_tpr_threshold(fp_rate, tp_rate)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:33.852063Z","iopub.execute_input":"2023-08-16T20:10:33.85286Z","iopub.status.idle":"2023-08-16T20:10:34.115916Z","shell.execute_reply.started":"2023-08-16T20:10:33.852817Z","shell.execute_reply":"2023-08-16T20:10:34.114815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __5.7. ROC AUC__\n\n__Key ideas:__\n\n1. This metric is very __useful when comparing models.__\n\n> __Perfect classifier: AUC = 1__\n\n> Random Classifier: AUC = 0.5 (as we see in the \"diagonal line\"of ROC Curve of the section \"5.6. ROC Curve\".\n\n2. ROC Curve AUC (Area Under the Curve)","metadata":{}},{"cell_type":"code","source":"#ROC AUC Score \nprint(\"\\n\", '\\033[1m','\\033[94m',\"ROC AUC Score:\",'\\033[0m', \"\\n\")\nprint(roc_auc_score(y_train_9, y_train_scores))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:34.117213Z","iopub.execute_input":"2023-08-16T20:10:34.117547Z","iopub.status.idle":"2023-08-16T20:10:34.137614Z","shell.execute_reply.started":"2023-08-16T20:10:34.117518Z","shell.execute_reply":"2023-08-16T20:10:34.136474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __6. Multiclass Classification__","metadata":{}},{"cell_type":"markdown","source":"## __6.1. Training Model (Multiclass Classifier) - Training Dataset__\n","metadata":{}},{"cell_type":"markdown","source":"__Key ideas (Part 1):__\n\n1. __Binary Classifier__: distinguish between two classes; __however, there are several ways to use multiple \"Binary Classifiers\" to do the task of \"Multiclass Classification\"__. Some models are:\n\n> Logistic Regression\n\n> Support Vector Machines\n\n> SGD Classifier (i.e., it is a \"Linear Classifier\" + application of the optimization technique called \"Stochastic Gradient Descent\").\n\n2. __Multiclass Classifier__: distinguish between two or more classes. Some models are:\n\n> SGD Classifier (i.e., it is a \"Linear Classifier\" + application of the optimization technique called \"Stochastic Gradient Descent\").\n\n> Random Forests Classifiers\n\n> Naive Bayes Classifiers","metadata":{}},{"cell_type":"markdown","source":"__Key ideas (Part 2):__\n\n1. Some ways to use \"Multiple Binary Classifiers\" to do the task of \"Multiclass Classification\" are \"One-vs-the-one (OvO) multiclass strategy\" and \"One-vs-the-rest (OvR) multiclass strategy\"\n\n2. One-vs-the-one (OvO) multiclass strategy: it is a multiclass classification technique that works by __training one classifier for each pair of classes in the data__.\n\n> __Each classifier is trained on a subset of the data that contains only the samples from the two classes it is comparing__. \n\n> At prediction time (i.e. when we apply the \"prediction\" commands), __each classifier votes for one of the two classes it knows about__, and the class with the most votes is selected as the final prediction. \n\n> This technique can be __useful for algorithms that do not scale well with the number of samples__ (i.e. algoirthms that handling a lot of observations results in too much time or too many computational resources being necessary to be used), such as __kernel methods__.\n\n> This strategy is usually slower than \"One-vs-the-rest (OvR) multiclass strategy\".\n\n3. Kernel algorithms: they are a class of machine learning algorithms that use a __\"kernel function (also called \"kernel method\")\" to map the input data into a higher-dimensional feature space__, where linear methods can be applied __to solve nonlinear problems.__\n\n> \"Kernel algorithms\" can be used for various tasks: classification, regression, clustering, dimensionality reduction, and anomaly detection.\n\n> Examples of \"kernel algorithms\" are: support vector machines, kernel perceptron, kernel principal component analysis, kernel k-means, and Gaussian processes. \n\n4. __Kernel functions measure the similarity between two data points.__ \n\n> Examples of \"kernel functions\": linear,polynomial, radial basis function (RBF), sigmoid, and Laplacian. \n\n5. One-vs-the-rest (OvR) multiclass strategy: it is a multiclass classification technique that __consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes._-\n\n> It is __computational efficient__ and easy to __interpret__ (since only \"n\" classes classifiers are needed).\n\n> __This is the most commonly used strategy for multiclass classification and is a fair default choice__","metadata":{}},{"cell_type":"code","source":"#One-vs-the-rest (OvR) multiclass strategy\n    #Model is going to be named as \"ovr_classifier\"\n\n    #We are going to see an explicit way to apply this model (fit the data and then make predictions)..\n    #...(i.e., One-vs-the-rest (OvR) multiclass strategy) \n    #Although we have the option to use Binary Algorithms such as \"Logistic Regression,...\n    #...Support Vector Machines or SGD Classifier\"\n    \n    #Extremely Important: I built 2 models (with different properties). The model that I am going\n    #...to use for performance checking is called \"Model N°1: ovr_classifier\":\n        #The data used for the \"fitting process\" in the following code are just the first 100 rows from...\n        #...\"the X training sets and Y training set\" because it will take at least 30 min (or even more)...\n        #...to use the entire \"X and Y training set\" (i.e., 42000 rows) for the \"fitting process\"...\n        #...in my laptop.\n        #The data used for the \"predicting process\" in the following code are rows in the range from...\n        #...[101:42001] from the \"X training sets\".\n    \n    #We are going to use the \"X training and Y training\" Numpy arrays generated in the..\n    #...section \"3.1. Adjusting the Training Dataset\".\n    \n    #SVC(): \n        #It means \"Support Vector Classification\"\n        #It will work as an \"estimator\" argument in the function \"OneVsRestClassifier()\"\n        \n    #As a way to check that the \"One-vs-the-rest (OvR) multiclass strategy\" is working properly, I...\n    #...use predict the value of the variable \"digitx\" (as I did in section...\n    #...\"3.1. Adjusting the Training Dataset\").\n    \n#Model N°1: ovr_classifier\"    \n    #Important: \"Performance Measures\" (described in section 7. Performance Measures - Training Model...\n    #...(Multiclass Classifier) - Training Dataset) are developed for this model; however this model...\n    #...was not selected for the \"Test Phase\".\novr_classifier = OneVsRestClassifier(SVC())\novr_classifier.fit(array_X[0:100],array_Y[0:100])\novr_classifier.predict(array_X[101:42001])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:34.138894Z","iopub.execute_input":"2023-08-16T20:10:34.139204Z","iopub.status.idle":"2023-08-16T20:10:49.252623Z","shell.execute_reply.started":"2023-08-16T20:10:34.139178Z","shell.execute_reply":"2023-08-16T20:10:49.251519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model N°2: ovr_classifier1\n    #Important: \"Performance Measures\" (described in section 7. Performance Measures - Training Model...\n    #...(Multiclass Classifier) - Training Dataset) are not developed for this model; however this model...\n    #...was selected for the \"Test Phase\".\novr_classifier1 = OneVsRestClassifier(SVC())\novr_classifier1.fit(array_X[0:20000],array_Y[0:20000])\novr_classifier1.predict(array_X[20000:42001])","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:10:49.253848Z","iopub.execute_input":"2023-08-16T20:10:49.254136Z","iopub.status.idle":"2023-08-16T20:16:50.723808Z","shell.execute_reply.started":"2023-08-16T20:10:49.254111Z","shell.execute_reply":"2023-08-16T20:16:50.722706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __7. Performance Measures - Training Model (Multiclass Classifier) - Training Dataset__","metadata":{}},{"cell_type":"markdown","source":"## __7.1. Confusion Matrix (One-vs-the-rest {OvR} multiclass strategy)__","metadata":{}},{"cell_type":"code","source":"#Step 1 (Cross-Validation in \"One-vs-the-rest {OvR} multiclass strategy\")\n    #It generate \"cross-validated estimates\" (also called \"pseudo-predictions\") for each input data point...\n    #...In other words, this function returns for \"each element in the input\", the \"prediction\" that...\n    #...was obtained for that \"element when it was in the test set\". during the \"Cross-Validation process\".\n        #In this step we are going to generate \"predictions\" using the training data.        \n        #Then those \"predictions\" will be compared to the known target values (because all data values..\n        #...used in this Step 1 belong to the \"training set\").\n    \n    #cv:\n        #The data is splitted according to the \"cv\" parameter. \n        #It is the \"cross-validation generator\" (default=None).\n        #Determines the \"cross-validation\" splitting strategy. \n        #Possible inputs for \"cv\" are:\n            #None (to use the default 5-fold cross validation).\n            #int (to specify the number of folds in a \"\"(Stratified)KFold\".\n        #If the \"estimator\" (also called model, which in our case is called \"ovr_classifier\") is a...\n        #...\"classifier\" and the \"dependent variable\" is either \"binary or multiclass\", the...\n        #...\"StratifiedKFold\" is used. \n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Predictions from CV Process in One-vs-the-rest {OvR} multiclass strategy:\",'\\033[0m', \"\\n\")\ny_train_pred_ovr = cross_val_predict(ovr_classifier, X_training[0:100], Y_training[0:100], cv=2)\nprint(y_train_pred_ovr)\n    \n#Step 2 (Confusion Matrix in \"One-vs-the-rest {OvR} multiclass strategy\")\n    #We are going to generate the \"Confusion Matrix\" using the \"training data\" and the \"predictions\"...\n    #...(from the previous step; i.e., from the Step 1)\n    \n    #Confusion Matrix\n        #Compute \"confusion matrix\" to evaluate the \"accuracy\" of a classification.\n        #The main diagonal shows us the \"true positives\" instances (i.e., correctly classified). Meanwhille..\n        #...the rest of the values of the matrix show us \"misclassified\" instances...\n        #...(i.e.,\"false negatives\" or \"false positives\").\n        \n#The output of the confusion matrix will shows us that there are:\n    # 16 images correctly classified  as 1s\n    # 9 images correctly classified  as 9s    \n    # 3 images of 5s misclassified  as 3s\n    # 2 images of 2s misclassified  as 6s\n    \nprint(\"\\n\", '\\033[1m','\\033[94m',\"Confusion Matrix in One-vs-the-rest {OvR} multiclass strategy:\",'\\033[0m', \"\\n\")\ncm_ovr = confusion_matrix(Y_training[0:100], y_train_pred_ovr[0:100])\nprint(cm_ovr)\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Plot of the Confusion Matrix in One-vs-the-rest {OvR} multiclass strategy:\",'\\033[0m', \"\\n\")\ndisp_confusion_matrix_ovr = ConfusionMatrixDisplay(confusion_matrix=cm_ovr)\ndisp_confusion_matrix_ovr.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:33.348069Z","iopub.execute_input":"2023-08-16T20:20:33.34845Z","iopub.status.idle":"2023-08-16T20:20:34.40591Z","shell.execute_reply.started":"2023-08-16T20:20:33.348421Z","shell.execute_reply":"2023-08-16T20:20:34.404803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __7.2. Confusion Matrix - Image Representation (One-vs-the-rest {OvR} multiclass strategy)__","metadata":{}},{"cell_type":"code","source":"#Confusion Matrix - Image Representation (One-vs-the-rest {OvR} multiclass strategy)\n    #The main diagonal shows us the \"true positives\" instances (i.e., correctly classified).\n    #The main diagonal shows us dark squares:\n        #It could mean poor performance of the model in that specific class compared to other classes\n        #It could mean few instances of that specific class\n    #The main diagonal shows us brighter squares:\n        #It could mean correctly classified data instances (good performance of the model...\n        #...compared to other classes)\n        \n#The output of \"Confusion Matrix - Image Representation\" show us in the \"Main diagonal\" a good...\n#...performance on \"1s class\" and apparently a regular performance in the other classes...\n#...(considering obviously that we are using only 100 instances of the training dataset and not...\n#...the entire training dataset).\nplt.matshow(cm_ovr,cmap=plt.cm.gray)\nplt.title(\"Confusion Matrix - Image Representation\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:34.407246Z","iopub.execute_input":"2023-08-16T20:20:34.408092Z","iopub.status.idle":"2023-08-16T20:20:35.03966Z","shell.execute_reply.started":"2023-08-16T20:20:34.408063Z","shell.execute_reply":"2023-08-16T20:20:35.038331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __7.3. Matrix of Error Rates - Image Representation (One-vs-the-rest {OvR} multiclass strategy)__","metadata":{}},{"cell_type":"code","source":"#Error Analysis - Image Representation\n\n    #This anaysis is derived from the technique...\n    #...\"Confusion Matrix\"- Image Representation (One-vs-the-rest {OvR} multiclass strategy)\n\n    #Step #1:Apply sum of the elements of an array (i.e., Numpy array) over a given axis.\n        #Axis = 1 (i.e. all elements in every row = elements that runs horizontally across the columns).\n    \n        #keepdims (optional): bool\n            #If this is set to \"True\", the axes which are reduced are left in the result as dimensions...\n            #...with size one.\n                #In this case the reduced axes is the \"axis 1\" (the columns); therefore the result will...\n                #...show us only one column.           \nprint(\"\\n\", '\\033[1m','\\033[94m',\"Sum of the elements of an array:\",'\\033[0m', \"\\n\")                \nrow_sums_ovr = cm_ovr.sum(axis=1,keepdims=True)\nprint(row_sums_ovr)\n\n    #Step #2: Finding the \"error rates\"\n        #We are going to use the \"misclassified instances\" (i.e., all the instances that do not belong...\n        #...to the main diagonal).\n        #We are going to divide every \"misclassified instance\" of the...\n        #...\"Confusion Matrix - One-vs-the-rest {OvR} multiclass strategy\" by the total number of...\n        #...instances that belong to each class (based again in the info of the...\n        #...\"Confusion Matrix\" - One-vs-the-rest {OvR} multiclass strategy.\n            #Considering this approach, the shape of the matrix \"cm_ovr\" {output of section...\n            #....7.1. Confusion Matrix (One-vs-the-rest {OvR} multiclass strategy} and the shape of...\n            #...the array \"row_sums_ovr\" (output of Step #1) are very important (i.e. both have...\n            #...the same quantity of rows at the  moment of the division).\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Matrix of Error Rates:\",'\\033[0m', \"\\n\")                    \nerror_cm_ovr = cm_ovr / row_sums_ovr\nprint(error_cm_ovr)\n\n    #Step #3: Plot on the errors\n        #The \"main diagonal\" of the matrix \"error_cm_ovr\" (output of Step #2) is fill wih zeros...\n        #...because the goal is to show the in the plot only the errors (i.e. the values that do not...\n        #...belong to the \"main diagonal\" of the matrix \"error_cm_ovr\" {output of Step #2}).\n        \n#The output of the \"Matrix of Error Rates - Image Representation\" show us the following:\n    #Bright square: From the point of view of \"Predicted Label\" (i.e., from a \"column point of view\"),...\n    #...there are a lot misclassified images as \"3s class\". \n    #Bright square: From the point of view of \"True Label\" (i.e., from a \"row point of view\"), ...\n    #...the model has troubles classifying true images of \"5s\" as \"5s class\". \nprint(\"\\n\", '\\033[1m','\\033[94m',\"Matrix of Error Rates - Image Representation:\",'\\033[0m', \"\\n\")                    \nnp.fill_diagonal(error_cm_ovr,0)\nplt.matshow(error_cm_ovr,cmap=plt.cm.gray)\nplt.title(\"Matrix of Error Rates - Image Representation\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:35.041307Z","iopub.execute_input":"2023-08-16T20:20:35.041646Z","iopub.status.idle":"2023-08-16T20:20:35.458006Z","shell.execute_reply.started":"2023-08-16T20:20:35.041617Z","shell.execute_reply":"2023-08-16T20:20:35.457148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __7.4. Classification Report (Confusion Matrix, Precision score, Recall score, and F1 score) - One-vs-the-rest (OvR) multiclass strategy__\n\n***Extremely Important: with this \"Classification Report\" code we can save a lot of time. To be specific, we could avoid doing the sections***:\n\n> Confusion Matrix\n\n> Precision and Recall\n\n> F1 Score\n\n1. __It will show us a summary of the most used performance metrics in classification__ algorithms. We will see:\n\n> Confusion Matrix, Precision score, Recall score, F1 score\n\n2. The __\"Confusion Matrix values\"__ are the same that we have seen in the section \"6.1. Confusion Matrix (One-vs-the-rest {OvR} multiclass strategy)\", to be specific the values of: \n\n> Support \"3s class\": (11 = 2+1+7+1)\n\n> Support \"9s class\": (14 = 2+2+1+9)","metadata":{}},{"cell_type":"code","source":"#Classification Report\n    #\"Labels\" of the \"Classification Report\"\nclass_labels_ovr = [\"0s class\",\"1s class\",\"2s class\",\"3s class\",\"4s class\",\"5s class\",\"6s class\",\"7s class\",\"8s class\",\"9s class\"]\n\nfrom sklearn.metrics import  classification_report\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Classification report in One-vs-the-rest {OvR} multiclass strategy:\",'\\033[0m', \"\\n\")\ncr_ovr = classification_report(Y_training[0:100], y_train_pred_ovr[0:100],target_names=class_labels_ovr)\nprint(cr_ovr)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:35.459497Z","iopub.execute_input":"2023-08-16T20:20:35.460076Z","iopub.status.idle":"2023-08-16T20:20:35.47497Z","shell.execute_reply.started":"2023-08-16T20:20:35.460045Z","shell.execute_reply":"2023-08-16T20:20:35.473795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __8. Evaluation of the Model - Test Dataset__","metadata":{}},{"cell_type":"markdown","source":"## __8.1. Adjusting the Test Dataset__","metadata":{}},{"cell_type":"code","source":"#Dataframe of features of the test dataset\nx_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Dataframe of test dataset\",'\\033[0m', \"\\n\")\nprint(x_test)\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Information (Shape and Type) of the Dataframe (X variable):\",'\\033[0m', \"\\n\")\n#Printing the \"shape\" of the independent variable\nprint(x_test.shape)\n\n#Printing the \"type\" of the independent variable\nprint(type(x_test))\n\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Information (Shape) of the Numpy Array (X variables):\",'\\033[0m', \"\\n\")\n#Transforming the \"independent variable\" (from Pandas Dataframe to Numpy Array)\narray_x_test = x_test.to_numpy()\nprint(array_x_test)\n\n#Printing the \"shape\" of the independent variable (as a Numpy Array)\nprint(array_x_test.shape) ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:35.476425Z","iopub.execute_input":"2023-08-16T20:20:35.476791Z","iopub.status.idle":"2023-08-16T20:20:37.334504Z","shell.execute_reply.started":"2023-08-16T20:20:35.476737Z","shell.execute_reply":"2023-08-16T20:20:37.333144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __8.2. Applying Multiclass Classification {One-vs-the-rest (OvR) Multiclass strategy} - Test Dataset__","metadata":{}},{"cell_type":"code","source":"#Applying the \"model\" learned from the training set.\n    #This model came from section \"6.1. Training Model (Multiclass Classifier) - Training Dataset\"\n    #This model is applied to the dataset that came from section \"8.1. Adjusting the Test Dataset\"\ntest_prediction = ovr_classifier1.predict(array_x_test)\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Array of the of predicted labels (test set)\",'\\033[0m', \"\\n\")\ntest_prediction","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:20:37.336251Z","iopub.execute_input":"2023-08-16T20:20:37.33679Z","iopub.status.idle":"2023-08-16T20:24:15.184926Z","shell.execute_reply.started":"2023-08-16T20:20:37.336727Z","shell.execute_reply":"2023-08-16T20:24:15.184094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## __8.3. Preparing the outputs to be submitted__","metadata":{}},{"cell_type":"code","source":"#Converting the \"NumPy array\" in a \"Dataframe\"\n    #The nump \"NumPy array\" came from section \"8.2. Applying Multiclass Classification...\n    #...{One-vs-the-rest (OvR) Multiclass strategy} - Test Dataset\"\ntest_prediction_df = pd.DataFrame(test_prediction, columns = ['Label'])\nprint(\"\\n\", '\\033[1m','\\033[94m',\"Dataframe of the of predicted labels (test set)\",'\\033[0m', \"\\n\")\nprint(test_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:25:35.164251Z","iopub.execute_input":"2023-08-16T20:25:35.164649Z","iopub.status.idle":"2023-08-16T20:25:35.173361Z","shell.execute_reply.started":"2023-08-16T20:25:35.164619Z","shell.execute_reply":"2023-08-16T20:25:35.172418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Putting the\" index\" as a \"column\"\ntest_prediction_df['index'] = test_prediction_df.index\ntest_prediction_df\n\n#Renaming the \"columns\"\ntest_prediction_df.rename(columns={'Label': 'Label', 'index': 'ImageId'}, inplace=True)\ntest_prediction_df\n\n#Swapping the \"columns\"\ncolumns_titles = [\"ImageId\",\"Label\"]\ntest_prediction_df=test_prediction_df.reindex(columns=columns_titles)\ntest_prediction_df","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:24:15.202582Z","iopub.execute_input":"2023-08-16T20:24:15.202954Z","iopub.status.idle":"2023-08-16T20:24:15.225555Z","shell.execute_reply.started":"2023-08-16T20:24:15.202914Z","shell.execute_reply":"2023-08-16T20:24:15.224468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\", '\\033[1m','\\033[94m',\"Submmitted Dataframe\",'\\033[0m', \"\\n\")\nprint(test_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:24:15.227023Z","iopub.execute_input":"2023-08-16T20:24:15.227358Z","iopub.status.idle":"2023-08-16T20:24:15.23588Z","shell.execute_reply.started":"2023-08-16T20:24:15.22733Z","shell.execute_reply":"2023-08-16T20:24:15.234738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tranforming the file from \"Dataframe\" (previously created) to a \"CSV File\"\n#Submission file format (according to Kaggle):  \n    #Submit a csv file with exactly 28000 entries plus a header row. \n    #Your submission will show an error if you have extra columns (beyond ImageId and Label) or rows.\nSubmission_Final = test_prediction_df.to_csv('Submission_Final.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:25:41.409218Z","iopub.execute_input":"2023-08-16T20:25:41.409982Z","iopub.status.idle":"2023-08-16T20:25:41.465443Z","shell.execute_reply.started":"2023-08-16T20:25:41.409948Z","shell.execute_reply":"2023-08-16T20:25:41.464403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __References__\n\n1. **Hands-on Machine Learning with Scikit Learn, Keras and TensorFlow (Second Edition)**\n> Aurélien Gerón","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:red\"> __Opportunities for enhancement__</span>\n\n1. Transformation of the data \n\n> Using Pipelines (Feature Scaling)\n\n2. Apply \"Data Augmentation\" (shifting or adding noise to images)","metadata":{}}]}